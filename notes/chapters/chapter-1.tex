\chapter{Linear Regression}

Linear Regression is a statistical method used for modeling the relationship between a dependent variable 
(target or output) and one or more independent variables (predictors or features).

\section{Linear Regression with Single Feature}
This regression deals with one independent variable ($x$).

\subsection{Linear Regression Model}

\begin{equation}
y = \theta_0^t + \theta_1^t x + \epsilon
\end{equation}

\begin{equation}
\hat{y} = h_\theta(x) = \theta_0 + \theta_1 x
\end{equation}


\noindent
where:
\begin{itemize}
    \item $y$:  the dependent variable (target)
    \item $x$: the independent variable or input feature used to predict \(y\).
    \item $\theta_1^t$:  the slope or coefficient of \(x\) at iteration \(t\)
    \item $\theta_0^t$: intercept at iteration $t$
    \item $\epsilon$: the error term or residual. It captures the noise or other unmodeled effects.
    \item $\hat{y}$: the output of the linear regression model () for a given input (\(x\))
    \item $h_\theta(x)$: the hypothesis function for linear regression.
\end{itemize}

\subsection{Cost Function}
The cost function for linear regression measures the average squared error between predicted and actual values. It is defined as:
\begin{equation}
J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left( \hat{y}^{(i)} - y^{(i)} \right)^2
= \frac{1}{2m} \sum_{i=1}^m \left( (\theta_0 + \theta_1 x^{(i)}) - y^{(i)} \right)^2
\end{equation}

\noindent
where:
\begin{itemize}
    \item $m$: Number of input and output datapoints.
    \item $\frac{1}{2}$: A factor kept for convenience, as it simplifies the derivative calculations
    during gradient descent.
\end{itemize}

\subsection{Minimizing Cost}

The error for each data point can be written as:

\begin{equation}
    e^{(i)} = y^{(i)} - (\theta_0 + \theta_1 x^{(i)})
\end{equation}

The cost function becomes:
\begin{equation}
    J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m e^{(i)^2}
\end{equation}

We need to find \(\theta_0\) and \(\theta_1\) that minimize \(J\). This requires setting the partial derivatives of
\(J\) with respect to \(\theta_0\) and \(\theta_1\) to zero as the cost function is quadratic and
there's just one critical point.
\\\\
\textbf{Solving for $\theta_0$:}
\begin{equation*}
    \frac{\partial J}{\partial \theta_0} = -\frac{1}{m} \sum_{i=1}^m \left( y^{(i)} - (\theta_0 + \theta_1 x^{(i)}) \right) = 0
\end{equation*}

\begin{gather*}
    \sum_{i=1}^m \left( y^{(i)} - (\theta_0 + \theta_1 x^{(i)}) \right) = 0\\
    \sum_{i=1}^m y^{(i)} = m\theta_0 + \theta_1 \sum_{i=1}^m x^{(i)}\\
\end{gather*}

Divide through by \(m\):

\[
\bar{y} = \theta_0 + \theta_1 \bar{x}
\]

where \(\bar{y}\) and \(\bar{x}\) are the means of \(y^{(i)}\) and \(x^{(i)}\), respectively.

Rearranging:
\begin{equation}
    \theta_0 = \bar{y} - \theta_1 \bar{x}
\end{equation}
\\\\
\textbf{Solving for $\theta_1$:}
\begin{equation*}
    \frac{\partial J}{\partial \theta_1} = -\frac{1}{m} \sum_{i=1}^m x^{(i)} \left( y^{(i)} -
    (\theta_0 + \theta_1 x^{(i)}) \right) = 0
\end{equation*}


Expanding:
\[
\sum_{i=1}^m x^{(i)} y^{(i)} = \theta_0 \sum_{i=1}^m x^{(i)} + \theta_1 \sum_{i=1}^m \left( x^{(i)} \right)^2
\]
Substitute \(\theta_0 = \bar{y} - \theta_1 \bar{x}\):
\[
\sum_{i=1}^m x^{(i)} y^{(i)} = (\bar{y} - \theta_1 \bar{x}) \sum_{i=1}^m x^{(i)} + \theta_1 \sum_{i=1}^m (x^{(i)})^2
\]
Simplify:
\[
\sum_{i=1}^m x^{(i)} y^{(i)} = \bar{y} \sum_{i=1}^m x^{(i)} - \theta_1 \bar{x} \sum_{i=1}^m x^{(i)} + \theta_1 \sum_{i=1}^m (x^{(i)})^2
\]
Reorganize terms:
\[
\theta_1 \left( \sum_{i=1}^m (x^{(i)})^2 - \frac{1}{m} \left( \sum_{i=1}^m x^{(i)} \right)^2 \right) = \sum_{i=1}^m x^{(i)} y^{(i)} - \frac{1}{m} \sum_{i=1}^m x^{(i)} \sum_{i=1}^m y^{(i)}
\]

Using simplified notation:
\begin{itemize}
    \item \( \bar{x} = \frac{1}{m} \sum_{i=1}^m x^{(i)} \) (mean of \(x\))
    \item \( \bar{y} = \frac{1}{m} \sum_{i=1}^m y^{(i)} \) (mean of \(y\))
\end{itemize}

\begin{equation}
\theta_1 = \frac{\sum_{i=1}^m \left( x^{(i)} - \bar{x} \right) \left( y^{(i)} - \bar{y} \right)}{\sum_{i=1}^m \left( x^{(i)} - \bar{x} \right)^2}
\end{equation}

\subsection{Gradient Descent Algorithm to find optimal $\theta_0$ and $\theta_1$}

\begin{enumerate}
    \item Gradient descent moves the parameters in the direction of the negative gradient (steepest descent)
    of the cost function.
    \item The learning rate $\alpha$ determines the size of each step. If $\alpha$ is too large, the algorithm
    may overshoot the minimum. If it is too small, convergence may take too long.
    \item The iterative process ensures gradual improvement in the model parameters until the cost function is minimized.
\end{enumerate}

\begin{algorithm}[h!]
\caption{Gradient Descent for Linear Regression}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Learning rate $\alpha$, initial values for $\theta_0$ and $\theta_1$, and maximum iterations or
convergence threshold $\epsilon$.
\STATE \textbf{Output:} Optimized parameters $\theta_0$ and $\theta_1$.
\STATE Set initial values for $\theta_0$ and $\theta_1$.
\REPEAT
    \STATE Compute updates for parameters:
    \begin{align*}
        \text{temp0} & := \theta_0 - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)} \right) \\
        \text{temp1} & := \theta_1 - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)} \right) x^{(i)}
    \end{align*}
    \STATE Update parameters simultaneously:
    \begin{align*}
        \theta_0 & := \text{temp0} \\
        \theta_1 & := \text{temp1}
    \end{align*}
    \STATE Compute cost function:
    \[
    J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m \left( \theta_0 + \theta_1 x^{(i)} - y^{(i)} \right)^2
    \]
\UNTIL {maximum iterations reached \textbf{or} change in $J(\theta_0, \theta_1)$ is below threshold $\epsilon$.}
\RETURN $\theta_0, \theta_1$
\end{algorithmic}
\end{algorithm}

\nocite{*}
\newpage
\section{Linear Regression with Multiple Features}

\begin{equation}
    y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n + e
\end{equation}

\noindent where
\begin{itemize}
    \item $y$: The dependent variable (or target/output variable) that the model predicts
    \item $\theta_0$: The intercept term, representing the value of \(y\) when all \(x_i = 0\)
    \item $\theta_1, \theta_2, \dots, \theta_n$: The coefficients or weights for the independent variables
    \(x_1, x_2, \dots, x_n\).
\end{itemize}

\subsection{General Form}

\begin{equation}
    y = \boldsymbol{\theta}^T \mathbf{x} + e
\end{equation}

where $ \boldsymbol{\theta} \in \mathbb{R}^{(n+1) \times 1}$ is the parameter vector (column vector of coefficients), defined as:

\begin{equation}
    \boldsymbol{\theta} =
     \begin{bmatrix}
     \theta_0 \\
     \theta_1 \\
     \vdots \\
     \theta_n
     \end{bmatrix}
\end{equation}

And the input feature vector $\mathbf{x} \in \mathbb{R}^{(n+1) \times 1}$  defined as:

\begin{equation}
    \mathbf{x} = \begin{bmatrix}
                     1 \\
                    x_1 \\
                    \vdots \\
                    x_n
    \end{bmatrix}
\end{equation}

The prediction function is:

\begin{equation}
    \hat{y} = h_\theta(\mathbf{x}) = \boldsymbol{\theta}^T \mathbf{x}
\end{equation}

\subsection{Cost Function}

The residuals (\(E\)) represent the difference between the actual target values (\(y\)) and the
predicted values (\(X \cdot \boldsymbol{\theta}\)). For \(m\) data points:

\begin{equation}
    E = \begin{bmatrix}
    e^{(1)} \\
    e^{(2)} \\
    \vdots \\
    e^{(m)}
    \end{bmatrix}
    = X \cdot \boldsymbol{\theta} - y
\end{equation}

The cost function \(J(\boldsymbol{\theta})\) is:

\begin{equation}
    J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^m \left( e^{(i)} \right)^2
\end{equation}

Using \(E = X \cdot \boldsymbol{\theta} - y\), we can write:
\[
J(\boldsymbol{\theta}) = \frac{1}{2m} \|E\|^2
\]

Expanding this using the transpose:
\[
J(\boldsymbol{\theta}) = \frac{1}{2m} E^T E
\]

Substituting \(E = X \cdot \boldsymbol{\theta} - y\):
\begin{equation}
J(\boldsymbol{\theta}) = \frac{1}{2m} (X \cdot \boldsymbol{\theta} - y)^T (X \cdot \boldsymbol{\theta} - y)
\end{equation}


To minimize \(J(\boldsymbol{\theta})\), we compute the gradient (partial derivative) with respect to each
parameter \(\theta_j\):

\begin{equation}
    \frac{\partial J(\boldsymbol{\theta})}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)}
\end{equation}

We now compute the partial derivative of \( J(\boldsymbol{\theta}) \) with respect to \( \theta_j \).
The chain rule will be used.

\[
J(\boldsymbol{\theta}) = \frac{1}{2m} \left[ (X \cdot \boldsymbol{\theta})^T (X \cdot \boldsymbol{\theta})
- 2 y^T (X \cdot \boldsymbol{\theta}) + y^T y \right]
\]

\[
J(\boldsymbol{\theta}) = \frac{1}{2m} \left[ \boldsymbol{\theta}^T X^T X \boldsymbol{\theta}
- 2 y^T X \boldsymbol{\theta} \right] + \text{constant terms independent of } \boldsymbol{\theta}
\]

Compute the derivative with respect to \( \boldsymbol{\theta} \) (matrix calculus):

\begin{itemize}
    \item Derivative of \( \boldsymbol{\theta}^T X^T X \boldsymbol{\theta} \):
    \[
     \frac{\partial}{\partial \boldsymbol{\theta}} \left( \boldsymbol{\theta}^T X^T X \boldsymbol{\theta} \right) = 2 X^T X \boldsymbol{\theta}
     \]
    \item Derivative of \( -2 y^T X \boldsymbol{\theta} \):
    \[
     \frac{\partial}{\partial \boldsymbol{\theta}} \left( -2 y^T X \boldsymbol{\theta} \right) = -2 X^T y
     \]
\end{itemize}

Substituting above values, we get:
\begin{equation}
\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = \frac{1}{m} \left( X^T X \boldsymbol{\theta} - X^T y \right)
\end{equation}


From the general gradient, we can isolate the derivative with respect to a single parameter \( \theta_j \). The gradient is:
\[
\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = \frac{1}{m} X^T (X \cdot \boldsymbol{\theta} - y)
\]

The \( j \)-th element of the gradient corresponds to:
\[
\frac{\partial J(\boldsymbol{\theta})}{\partial \theta_j} = \frac{1}{m} (X_{\text{column}, j})^T (X \cdot \boldsymbol{\theta} - y)
\]

\subsection{Gradient of the Cost Function}

The gradient of the cost function with respect to all parameters \(\boldsymbol{\theta}\) is a vector:

\begin{equation}
    \text{grad} = \begin{bmatrix}
\frac{\partial J(\boldsymbol{\theta})}{\partial \theta_0} \\
\frac{\partial J(\boldsymbol{\theta})}{\partial \theta_1} \\
\vdots \\
\frac{\partial J(\boldsymbol{\theta})}{\partial \theta_n}
\end{bmatrix}
\end{equation}

\begin{equation}
    \text{grad} = \frac{1}{m} X^T (X \cdot \boldsymbol{\theta} - y)
\end{equation}

\subsection{Analytical Solution}

To find the optimal \(\boldsymbol{\theta}\), set the gradient \(\text{grad}\) to zero:

\[
\frac{1}{m} X^T (X \cdot \boldsymbol{\theta} - y) = 0
\]

Simplify:
\[
X^T (X \cdot \boldsymbol{\theta}) = X^T y
\]

Rearranging:
\begin{equation}
\boldsymbol{\theta} = (X^T X)^{-1} X^T y
\end{equation}

This is known as the normal equation, which gives the closed-form solution for \(\boldsymbol{\theta}\) in
linear regression.

\subsection{Gradient Descent Algorithm}

\begin{algorithm}[h!]
\caption{Gradient Descent for Linear Regression for multiple features}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Learning rate $\alpha$, initial values for $\boldsymbol{\theta}$, and maximum iterations or
convergence threshold $\epsilon$.
\STATE \textbf{Output:} Optimized parameter vector, $\boldsymbol{\theta}$.
\REPEAT
    \STATE Compute gradient vector: $\textbf{grad} \in \mathbb{R}^{(n+1) \times 1}$:
    \[
        \textbf{grad} = X^T (X \cdot \boldsymbol{\theta} - y)
    \]
    \STATE Update the parameter vector, $\boldsymbol{\theta}$:
    \[
        \boldsymbol{\theta} := \boldsymbol{\theta} - \alpha \cdot \textbf{grad}
    \]
    \STATE Compute cost function (just for monitoring):
    \[
    J(\boldsymbol{\theta}) = \frac{1}{2m} (X \cdot \boldsymbol{\theta} - y)^T (X \cdot \boldsymbol{\theta} - y)
    \]
\UNTIL {maximum iterations reached \textbf{or} change in $J(\boldsymbol{\theta})$ is below threshold $\epsilon$.}
\RETURN $\boldsymbol{\theta}$
\end{algorithmic}
\end{algorithm}

\section{Linear Models for Regression}

\subsection{Polynomial Curve Fitting}

\begin{equation}
    \hat{y}(x, \boldsymbol{\theta}) = \theta_0 + \theta_1 x + \theta_2 x^2 + \dots + \theta_n x^n
\end{equation}

\begin{equation}
\text{Let } X =
\begin{bmatrix}
1 & x^{(1)} & (x^{(1)})^2 & (x^{(1)})^3 & \cdots & (x^{(1)})^n \\
1 & x^{(2)} & (x^{(2)})^2 & (x^{(2)})^3 & \cdots & (x^{(2)})^n \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x^{(m)} & (x^{(m)})^2 & (x^{(m)})^3 & \cdots & (x^{(m)})^n
\end{bmatrix}
\end{equation}


and the target vector:
\begin{equation}
y =
\begin{bmatrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)}
\end{bmatrix}
\end{equation}


\subsection{Analytical Solution}

Just like previous muti-feature solution:

\begin{equation}
\boldsymbol{\theta}^* = (X^T X)^{-1}X^T \mathbf{y}
\end{equation}

\subsection{Linear Models with Basis Functions}
\begin{equation}
    \hat{y} = h_{\theta}(\mathbf{x}) = \boldsymbol{\theta}^T \boldsymbol{\Phi}(\mathbf{x})
    = \sum_{j=0}^{n} \theta_j \phi_j(\mathbf{x})
\end{equation}

where $\boldsymbol{\Phi}(\mathbf{x})$ is a matrix of basis functions:

\begin{equation}
    \boldsymbol{\Phi} =
\begin{bmatrix}
\phi_0(x^{(1)}) & \phi_1(x^{(1)}) & \phi_2(x^{(1)}) & \cdots & \phi_n(x^{(1)}) \\
\phi_0(x^{(2)}) & \phi_1(x^{(2)}) & \phi_2(x^{(2)}) & \cdots & \phi_n(x^{(2)}) \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\phi_0(x^{(m)}) & \phi_1(x^{(m)}) & \phi_2(x^{(m)}) & \cdots & \phi_n(x^{(m)})
\end{bmatrix}
\end{equation}

The cost function is:
\[
    J(\theta) = \frac{1}{2m} \sum_{i=1}^m \left( e^{(i)} \right)^2
\]

Using the matrix form, the residuals for all data points can be written as \(E = \Phi \cdot \boldsymbol{\theta} - y\), and the cost function becomes:
\[
J(\theta) = \frac{1}{2m} \mathbf{E}^T \mathbf{E} = \frac{1}{2m} (\boldsymbol{\Phi} \cdot \boldsymbol{\theta} - \mathbf{y})^T
(\boldsymbol{\Phi} \cdot \boldsymbol{\theta} - \mathbf{y})
\]

The gradient of \(J(\theta)\) with respect to the parameters \(\boldsymbol{\theta}\) is:
\[
\text{grad} = \frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} =
\begin{bmatrix}
\frac{\partial J(\boldsymbol{\theta})}{\partial \theta_0} \\
\frac{\partial J(\boldsymbol{\theta})}{\partial \theta_1} \\
\vdots \\
\frac{\partial J(\boldsymbol{\theta})}{\partial \theta_n}
\end{bmatrix}
= \frac{1}{m} \boldsymbol{\Phi}^T \cdot (\boldsymbol{\Phi} \cdot \boldsymbol{\theta} - \mathbf{y})
\]



From the cost function:
\[
J(\theta) = \frac{1}{2m} (\boldsymbol{\Phi} \cdot \boldsymbol{\theta} - \mathbf{y})^T (\boldsymbol{\Phi} \cdot \boldsymbol{\theta} - \mathbf{y})
\]

Taking the derivative with respect to \(\boldsymbol{\theta}\):
\[
\text{grad} = \frac{1}{m} \boldsymbol{\Phi}^T \cdot (\boldsymbol{\Phi} \cdot \boldsymbol{\theta} - \mathbf{y})
\]

To find the optimal \(\boldsymbol{\theta}^*\), we set the gradient to zero:
\[
\frac{\partial J(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0
\]

This gives the normal equation:
\[
\boldsymbol{\Phi}^T \cdot \boldsymbol{\Phi} \cdot \boldsymbol{\theta} = \boldsymbol{\Phi}^T \cdot \mathbf{y}
\]

Rearranging:
\[
\boldsymbol{\theta}^* = (\boldsymbol{\Phi}^T \cdot \boldsymbol{\Phi})^{-1} \cdot \boldsymbol{\Phi}^T \cdot \mathbf{y}
\]


Gradient Descent Update Rule:

\[
\boldsymbol{\theta} := \boldsymbol{\theta} - \alpha \cdot \text{grad}
\]

Substituting the gradient:
\[
\boldsymbol{\theta} := \boldsymbol{\theta} - \alpha \cdot \frac{1}{m} \boldsymbol{\Phi}^T \cdot (\boldsymbol{\Phi} \cdot
\boldsymbol{\theta} - \mathbf{y})
\]

Here the \(\alpha\) is the learning rate, controlling the step size.

\section{Probabilistic Interpretation of Linear Regression}

Treating the target variable \(y\) as a random variable conditioned on the input features \(\mathbf{x}\), with the
assumption that the observed \(y\) values are drawn from a probability distribution centered around the model's
prediction.

\subsection{Equivalence of least square error and maximum likelihood estimation}
\[
    y = \boldsymbol{\theta}^T \mathbf{x} + e
\]

Here we have:
\begin{itemize}
    \item \( y \): The observed output (dependent variable or target)
    \item \( \boldsymbol{\theta} \): is the parameter vector
    \item \( \mathbf{x} \): is the feature/input vector
    \item \( e \): The random noise term, representing unmodeled or unexplained effects
\end{itemize}

For $i^{th}$ data point:
\[
    y^{(i)} = \boldsymbol{\theta}^T \mathbf{x}^{(i)} + e^{(i)}
\]

\noindent
\textbf{Model Assumptions:}

\begin{itemize}
    \item The relationship between the features (\(\mathbf{x}\)) and the target (\(y\)) is linear:
     \[
     \mu_y = h_\theta(\mathbf{x}) = \boldsymbol{\theta}^T \mathbf{x},
     \]
     where \( \mu_y \) is the mean of \(y\), predicted by the linear model.
    \item Observations \(y\) deviate from the linear prediction due to noise or unmodeled effects, captured by an additive random error term \(e\):
     \[
     y = \boldsymbol{\theta}^T \mathbf{x} + e.
     \]
   The noise \(e\) is assumed to follow a Gaussian distribution:
     \[
     e \sim \mathcal{N}(0, \sigma^2),
     \]
     where \(\sigma^2\) is the variance of the noise.
    \item Given \(\mathbf{x}\), \(y\) is conditionally normally distributed:
     \[
     y \mid \mathbf{x} \sim \mathcal{N}(\boldsymbol{\theta}^T \mathbf{x}, \sigma^2).
     \]
\end{itemize}

\noindent
\textbf{Probabilistic Model}:

The probability density function (PDF) of \(y\) given \(\mathbf{x}\) is:

\begin{equation}
p(y \mid \mathbf{x}, \boldsymbol{\theta}, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(y -
\boldsymbol{\theta}^T \mathbf{x})^2}{2\sigma^2}\right).
\end{equation}
\\

This represents the likelihood of observing a particular \(y\) value for a given input \(\mathbf{x}\). Now we can estimate the parameters \(\boldsymbol{\theta}\) and \(\sigma^2\) by maximizing the likelihood of the
observed data. For a dataset \(\{(\mathbf{x}^{(i)}, y^{(i)})\}_{i=1}^m\), the likelihood is the joint probability of all $m$ observations:

\begin{equation}
    \mathcal{L}(\boldsymbol{\theta}, \sigma^2) = \prod_{i=1}^m p(y^{(i)} \mid \mathbf{x}^{(i)}, \boldsymbol{\theta}, \sigma^2)
    = \prod_{i=1}^m \frac{1}{\sqrt{2 \pi \sigma}} \exp\left(-\frac{\left(y^{(i)} - h_\theta\left(x^{(i)}\right)\right)^2}{2 \sigma^2}\right)
\end{equation}

Taking $\ln$ both sides:

\begin{equation}
    \ell(\boldsymbol{\theta}, \sigma^2) = \ln \mathcal{L}(\boldsymbol{\theta}, \sigma^2)
= -\frac{m}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^m \left( y^{(i)} - h_\theta\left(\mathbf{x}^{(i)}\right) \right)^2
\end{equation}

Here, maximize the log-likelihood with respect to \(\boldsymbol{\theta}\) is equivalent to minimizing the sum of squared
errors, under the assumption of Gaussian distribution for the error:

\begin{equation}
    \boldsymbol{\theta}^* = \arg\min_{\boldsymbol{\theta}} \sum_{i=1}^m \left( y^{(i)} - \boldsymbol{\theta}^T \mathbf{x}^{(i)} \right)^2.
\end{equation}

This leads to the familiar solution of linear regression (normal equation):
\begin{equation}
    \boldsymbol{\theta}^* = (\Phi^T \Phi)^{-1} \Phi^T \mathbf{y}
\end{equation}

Thus, the optimal value of $\sigma^2$ can be calculated as:
\begin{equation}
    \sigma^2_{ML} = \frac{1}{m} \sum_{i=1}^m \left( y^{(i)} - h_{\theta_{ML}}\left(\mathbf{x}^{(i)}\right) \right)^2
\end{equation}

which is equivalent to minimized cost function.

For any new input \(\mathbf{x}\), the predicted value \(\hat{y}\) is computed using \(h_{\theta_{ML}}(\mathbf{x})\).